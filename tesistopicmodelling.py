# -*- coding: utf-8 -*-
"""TesisTopicModelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dDLTI9V_VeJtwMUAF-6BtJDLD0i0Oac7

### **Notebook para experimentación Tesis de grado**
**John Rincon**
**Tutor Haydemar Nuñez**

Este experimento se compone de:

1.   Carga de librerías y datos
2.   Preprocesamiento
3.   Matrices de representación
4.   SVD Singular vector descoposition
5.   pLSA Probabilistic Latent Semantic Analysis
6.   LDA Latent Dirilchelt Allocation
7.   Neural Networks Autoencoders
8.   Neural Networks Maquinas de Boltzmann
9.   Resultados de experimento (Métricas)
10.  Comparativo
11.  Conclusiones

# 1. **Carga de librerías y datos**
"""

pip install plsa

#import de librerías
import pandas as pd
import re , io
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

#Procesamiento de lenguaje natural
import nltk
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from nltk.corpus import stopwords

#Métricas de Coherencia
import gensim
from gensim.test.utils import common_corpus, common_dictionary
from gensim.corpora.dictionary import Dictionary
from gensim.models.coherencemodel import CoherenceModel

#plsa
from plsa.algorithms import PLSA
from plsa import Corpus, Pipeline, Visualize
from plsa.pipeline import DEFAULT_PIPELINE

#LDA
from nltk.tokenize import word_tokenize
from sklearn.decomposition import LatentDirichletAllocation as LDA

#autoencoders
import keras
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
from keras import layers, Sequential
from keras.layers import Dense, Activation, concatenate, Embedding, Input, LSTM, RepeatVector, TimeDistributed
from keras.models import Model
from keras import regularizers

#RBM
from sklearn.neural_network import BernoulliRBM

from wordcloud import WordCloud

#Es necesario definir el numero de tópicos que se van a trabajar en el experimiento
#En este caso se utilizan 5 grupos de tópicos y 7 palabras por tópicos
n_topics = 2
n_words = 6
tiempos = []

#funciones
#Diccionario de stopwords en español
nltk.download('stopwords')
spanish_stopwords = stopwords.words('spanish')
newStopWords = []
with open('/content/drive/MyDrive/TrainData/authors/spanishST.txt', errors='replace') as my_file:
    for line in my_file:
        newStopWords.append(line)
spanish_stopwords.extend(newStopWords)

#documentos pLSA
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
#Para tokenización LDA
nltk.download('punkt')

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

def print_topics(model, count_vectorizer, n_top_words):
    words = count_vectorizer.get_feature_names()
    for topic_idx, topic in enumerate(model.components_):
        print("\nTopic #%d:" % topic_idx)
        print(" ".join([words[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
def chunkIt(seq, num):
    avg = len(seq) / float(num)
    out = []
    last = 0.0

    while last < len(seq):
        out.append(seq[int(last):int(last + avg)])
        last += avg

    return out

#carga de datos
df = pd.read_excel('/content/drive/MyDrive/TrainData/authors/ds-TopicModelling.xlsx')

#Tamaño del set de datos
df.shape

#Revisión de Columnas
df.describe()

#Ver valores únicos en autores
df['autor'].unique()

#dividir dataset en autores
dfDC = df[df['autor'] == 'Daniel Coronell']
print("Tamaño DF Daniel Coronell", dfDC.shape)
dfED = df[df['autor'] == 'Enrique Dans']
print("Tamaño DF Enrique Dans", dfED.shape)
dfVM = df[df['autor'] == 'Victor Matín']
print("Tamaño DF Vitor Martín", dfVM.shape)
dfIG = df[df['autor'] == 'Isma García']
print("Tamaño DF Isma García", dfIG.shape)

"""Tamaño DF Autor 1 (777, 4)

Tamaño DF Autor 2 (1337, 4)

Tamaño DF Autor 3 (2568, 4)

Tamaño DF Autor 4 (2424, 4)

# 2. **Preprocesamiento**

## 2.1 Autor 1
"""

# Remover puntuación, caracteres especiales, mayusculas, urls y emoji
#Eliminar URLS
dfDC['texto_procesado'] = dfDC['texto'].apply(lambda x: re.split('https:\/\/.*', str(x))[0])
#cambiar a minuscula
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: x.lower())
#Remover stopwords y crear nueva columna 
pat = r'\b(?:{})\b'.format('|'.join(spanish_stopwords))
dfDC['texto_procesado'] = dfDC['texto_procesado'].str.replace(pat, '')
# Convertir las tildes a palabras minusculas
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('á', 'a', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('é', 'e', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('í', 'i', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('ó', 'o', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('ú', 'u', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('ü', 'u', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('ñ', 'n', x))
#Eliminar hashtags
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub(r'(?i)\#\w+', '', x))
#Eliminar Emojis
dfDC['texto_procesado'] = dfDC['texto_procesado'].str.replace(r'[^\x00-\x7F]+', '', regex=True)
#Eliminar mentions
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x))
#Remover espacios
dfDC['texto_procesado'] = dfDC['texto_procesado'].apply(lambda x: re.sub('  ', ' ', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].apply(lambda x: re.sub('   ', ' ', x))
#Remover puntuación y caracteres especiales
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('[¡!@#$:).;,¿?&\"|],', '', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub(', ', '', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub(' ,', '', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub(' ;', '', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('; ', '', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub(' :', '', x))
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub(': ', '', x))
#eliminar números
dfDC['texto_procesado'] = dfDC['texto_procesado'].map(lambda x: re.sub('[0-9]+', '', x))
#Remover stopwords y crear nueva columna 
#pat = r'\b(?:{})\b'.format('|'.join(spanish_stopwords))
dfDC['text_without_stopwords'] = dfDC['texto_procesado'].str.replace(pat, '')

#Ver primeras lineas
dfDC['texto_procesado'].head()

"""## 2.2 Autor 2

"""

# Remover puntuación, caracteres especiales, mayusculas, urls y emoji
#Eliminar URLS
dfED['texto_procesado'] = dfED['texto'].apply(lambda x: re.split('https:\/\/.*', str(x))[0])

#Minuscula
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: x.lower())

#Remover stopwords y crear nueva columna 
pat = r'\b(?:{})\b'.format('|'.join(spanish_stopwords))
dfED['texto_procesado'] = dfED['texto_procesado'].str.replace(pat, '')

# Convertir las tildes a palabras minusculas
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('á', 'a', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('é', 'e', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('í', 'i', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('ó', 'o', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('ú', 'u', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('ü', 'u', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('ñ', 'n', x))

#Eliminar hashtags
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub(r'(?i)\#\w+', '', x))

#Eliminar Emojis
dfED['texto_procesado'] = dfED['texto_procesado'].str.replace(r'[^\x00-\x7F]+', '', regex=True)

#Eliminar mentions
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x))

#Remover espacios
dfED['texto_procesado'] = dfED['texto_procesado'].apply(lambda x: re.sub('  ', ' ', x))
dfED['texto_procesado'] = dfED['texto_procesado'].apply(lambda x: re.sub('   ', ' ', x))

#Remover puntuación y caracteres especiales
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('[¡!@#$:).;,¿?&\"|],', '', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub(', ', '', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub(' ,', '', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub(' ;', '', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('; ', '', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub(' :', '', x))
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub(': ', '', x))

#eliminar números
dfED['texto_procesado'] = dfED['texto_procesado'].map(lambda x: re.sub('[0-9]+', '', x))

#Remover stopwords y crear nueva columna 
#pat = r'\b(?:{})\b'.format('|'.join(spanish_stopwords))
dfED['text_without_stopwords'] = dfED['texto_procesado'].str.replace(pat, '')

#Ver primeras lineas
dfED['texto_procesado'].head()

"""## 2.3 Autor 3

"""

# Remover puntuación, caracteres especiales, mayusculas, urls y emoji
#Eliminar URLS
dfVM['texto_procesado'] = dfVM['texto'].apply(lambda x: re.split('https:\/\/.*', str(x))[0])

# Convertir a palabras minusculas
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: x.lower())

#Remover stopwords y crear nueva columna 
pat = r'\b(?:{})\b'.format('|'.join(spanish_stopwords))
dfVM['texto_procesado'] = dfVM['texto_procesado'].str.replace(pat, '')

# Convertir las tildes a palabras minusculas
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: x.lower())
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('á', 'a', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('é', 'e', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('í', 'i', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('ó', 'o', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('ú', 'u', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('ü', 'u', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('ñ', 'n', x))

#Eliminar hashtags
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub(r'(?i)\#\w+', '', x))

#Eliminar Emojis
dfVM['texto_procesado'] = dfVM['texto_procesado'].str.replace(r'[^\x00-\x7F]+', '', regex=True)

#Eliminar mentions
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x))

#Remover espacios
dfVM['texto_procesado'] = dfVM['texto_procesado'].apply(lambda x: re.sub('  ', ' ', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].apply(lambda x: re.sub('   ', ' ', x))

#Remover puntuación y caracteres especiales
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('[¡!@#$:).;,¿?&\"|],', '', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub(', ', '', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub(' ,', '', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub(' ;', '', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('; ', '', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub(' :', '', x))
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub(': ', '', x))

#eliminar números
dfVM['texto_procesado'] = dfVM['texto_procesado'].map(lambda x: re.sub('[0-9]+', '', x))

#Remover stopwords y crear nueva columna 
#pat = r'\b(?:{})\b'.format('|'.join(spanish_stopwords))
dfVM['text_without_stopwords'] = dfVM['texto_procesado'].str.replace(pat, '')

#Ver primeras lineas
dfVM['texto_procesado'].head()

"""## 2.4 Autor 4

"""

# Remover puntuación, caracteres especiales, mayusculas, urls y emoji
#Eliminar URLS
dfIG['texto_procesado'] = dfIG['texto'].apply(lambda x: re.split('https:\/\/.*', str(x))[0])

# Convertir a palabras minusculas
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: x.lower())

#Remover stopwords y crear nueva columna 
pat = r'\b(?:{})\b'.format('|'.join(spanish_stopwords))
dfIG['texto_procesado'] = dfIG['texto_procesado'].str.replace(pat, '')

# Convertir las tildes a palabras minusculas
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: x.lower())
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('á', 'a', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('é', 'e', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('í', 'i', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('ó', 'o', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('ú', 'u', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('ü', 'u', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('ñ', 'n', x))

#Eliminar hashtags
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub(r'(?i)\#\w+', '', x))

#Eliminar Emojis
dfIG['texto_procesado'] = dfIG['texto_procesado'].str.replace(r'[^\x00-\x7F]+', '', regex=True)

#Eliminar mentions
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x))

#Remover espacios
dfIG['texto_procesado'] = dfIG['texto_procesado'].apply(lambda x: re.sub('  ', ' ', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].apply(lambda x: re.sub('   ', ' ', x))

#Remover puntuación y caracteres especiales
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('[¡!@#$:).;,¿?&\"|],', '', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub(', ', '', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub(' ,', '', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub(' ;', '', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('; ', '', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub(' :', '', x))
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub(': ', '', x))

#eliminar números
dfIG['texto_procesado'] = dfIG['texto_procesado'].map(lambda x: re.sub('[0-9]+', '', x))

#Remover stopwords y crear nueva columna 
#pat = r'\b(?:{})\b'.format('|'.join(spanish_stopwords))
dfIG['text_without_stopwords'] = dfIG['texto_procesado'].str.replace(pat, '')

#Ver primeras lineas
dfIG['texto_procesado'].head()

"""## 2.5 **Tokenización & Diccionarios**

### 2.5.1 **Autor 1**
"""

#Tokenización Autor 1
tokenizerDC = Tokenizer(split=' ')
tokenizerDC.fit_on_texts(dfDC['texto_procesado'].values)
#Crear diccionario Autor 1
tokenDC = list(sent_to_words(dfDC['text_without_stopwords']))
dictDC = Dictionary(tokenDC)
#dictDC.add_documents([["interpretaciones"], "autoriza"]])
corpusDC = [dictDC.doc2bow(text) for text in tokenDC]

"""### 2.5.2 **Autor 2**"""

#Tokenización Autor 2
tokenizerED = Tokenizer(split=' ')
tokenizerED.fit_on_texts(dfED['texto_procesado'].values)
#Crear diccionario Autor 1
tokenED = list(sent_to_words(dfED['text_without_stopwords']))
dictED = Dictionary(tokenED)
corpusED = [dictED.doc2bow(text) for text in tokenED]

"""### 2.5.3 **Autor 3**"""

#Tokenización Autor 3
tokenizerVM = Tokenizer(split=' ')
tokenizerVM.fit_on_texts(dfVM['texto_procesado'].values)
#Crear diccionario Autor 3
tokenVM = list(sent_to_words(dfVM['text_without_stopwords']))
dictVM = Dictionary(tokenVM)
corpusVM = [dictVM.doc2bow(text) for text in tokenVM]

"""### 2.5.4 **Autor 4**"""

#Tokenización Autor 4
tokenizerIG = Tokenizer(split=' ')
tokenizerIG.fit_on_texts(dfIG['texto_procesado'].values)
#Crear diccionario Autor 4
tokenIG = list(sent_to_words(dfIG['text_without_stopwords']))
dictIG = Dictionary(tokenIG)
corpusIG = [dictIG.doc2bow(text) for text in tokenIG]

"""# 3. **Matrices de representación**

## 3.1 **Bag of words**

### 3.1.1 Autor 1
"""

# Creando bag of words con Sklearn primer autor
countVecDC = CountVectorizer(stop_words = spanish_stopwords)
countDC = countVecDC.fit_transform(dfDC['texto_procesado'])
#Aquí se obtiene df de bolsa de palabras, conteo por sentencia
dfDC_bow = pd.DataFrame(countDC.toarray(),columns=countVecDC.get_feature_names())
#Transformamos a binario 1= está 0 = no está
dfDC_binary = (dfDC_bow > 0).astype(int)
print("Representación Bag of Words - Frecuencia Absoluta")
print(dfDC_bow)
print("Representación binaria")
print(dfDC_binary)

"""### 3.1.2 Autor 2"""

# Creando bag of words con Sklearn segundo autor
countVecED = CountVectorizer(stop_words = spanish_stopwords)
countED = countVecED.fit_transform(dfED['texto_procesado'])
#Aquí se obtiene df de bolsa de palabras, conteo por sentencia
dfED_bow = pd.DataFrame(countED.toarray(),columns=countVecED.get_feature_names())
#Transformamos a binario 1= está 0 = no está
dfED_binary = (dfED_bow > 0).astype(int)
print("Representación Bag of Words")
print(dfED_bow)
print("Representación binaria")
print(dfED_binary)

"""### 3.1.3 Autor 3"""

# Creando bag of words con Sklearn tercer autor
countVecVM = CountVectorizer(stop_words = spanish_stopwords)
countVM = countVecVM.fit_transform(dfVM['texto_procesado'])
#Aquí se obtiene df de bolsa de palabras, conteo por sentencia
dfVM_bow = pd.DataFrame(countVM.toarray(),columns=countVecVM.get_feature_names())
#Transformamos a binario 1= está 0 = no está
dfVM_binary = (dfVM_bow > 0).astype(int)
print("Representación Bag of Words")
print(dfVM_bow)
print("Representación binaria")
print(dfVM_binary)

"""### 3.1.4 Autor 4"""

# Creando bag of words con Sklearn cuarto autor
countVecIG = CountVectorizer(ngram_range=(1,1), stop_words = spanish_stopwords)
countIG = countVecIG.fit_transform(dfIG['texto_procesado'])
#Aquí se obtiene df de bolsa de palabras, conteo por sentencia
dfIG_bow = pd.DataFrame(countIG.toarray(),columns=countVecIG.get_feature_names())
#Transformamos a binario 1= está 0 = no está
dfIG_binary = (dfIG_bow > 0).astype(int)
print("Representación Bag of Words")
print(dfIG_bow)
print("Representación binaria")
print(dfIG_binary)

"""## 3.2 **TF IDF**"""

#Aplicando matriz de termino frecuencia e inversa a autores
#Aquí se define la matriz TF UIDF usando la libreria TfidfVectorizer de sklearn
tf_idf = TfidfVectorizer(ngram_range=(1,1), stop_words = spanish_stopwords)

"""### 3.2.1 Author 1"""

#Transformación a matriz de frecuencia de palabras
tf_idf_DC = tf_idf.fit_transform(dfDC['texto_procesado'])
#Crear dataframe con resultados
dfDC_tfidf = pd.DataFrame(tf_idf_DC.toarray(),columns=tf_idf.get_feature_names())
dfDC_tfidf

"""### 3.2.2 Author 2"""

#Transformación a matriz de frecuencia de palabras
tf_idf_ED = tf_idf.fit_transform(dfED['texto_procesado'])
#Crear dataframe con resultados
dfED_tfidf = pd.DataFrame(tf_idf_ED.toarray(),columns=tf_idf.get_feature_names())
dfED_tfidf

"""### 3.2.3 Author 3"""

#Transformación a matriz de frecuencia de palabras
tf_idf_VM = tf_idf.fit_transform(dfVM['texto_procesado'])
#Crear dataframe con resultados
dfVM_tfidf = pd.DataFrame(tf_idf_VM.toarray(),columns=tf_idf.get_feature_names())
dfVM_tfidf

"""### 3.2.4 Author 4"""

#Transformación a matriz de frecuencia de palabras
tf_idf_IG = tf_idf.fit_transform(dfIG['texto_procesado'])
#Crear dataframe con resultados
dfIG_tfidf = pd.DataFrame(tf_idf_IG.toarray(),columns=tf_idf.get_feature_names())
dfIG_tfidf

"""# 4. **SVD Singular vector descoposition**

## 4.1 Author 1

### 4.1.1 Representación Binaria
"""

# SVD modelo y transformación del autor 1 representación Binária
start = datetime.now()
svdDC_binary = TruncatedSVD(n_components=n_topics, random_state=64)
svdDC_binary.fit_transform(dfDC_binary)
end = datetime.now()
record = ["SVD Binary A1: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecDC.get_feature_names()
topDCSVDBinary = []
for i, comp in enumerate(svdDC_binary.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topDCSVDBinary.append(t[0])
        print(t[0])
        print(" ")

"""### 4.1.2 Representación Frecuencia TF IDF"""

# SVD modelo y transformación del autor 1 representación frecuencias
start = datetime.now()
svdDC_tfidf = TruncatedSVD(n_components=n_topics, random_state=64)
svdDC_tfidf.fit_transform(dfDC_tfidf)
end = datetime.now()
record = ["SVD TF-IDF A1: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecDC.get_feature_names()
topDCSVDtfidf = []
for i, comp in enumerate(svdDC_tfidf.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topDCSVDtfidf.append(t[0])
        print(t[0])
        print(" ")

"""## 4.2 Author 2

### 4.2.1 Representación Binaria
"""

# SVD modelo y transformación del autor 1 representación Binária
start = datetime.now()
svdED_binary = TruncatedSVD(n_components=n_topics, random_state=64)
svdED_binary.fit_transform(dfED_binary)
end = datetime.now()
record = ["SVD Binary A2: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecED.get_feature_names()
topEDSVDBinary = []
for i, comp in enumerate(svdED_binary.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topEDSVDBinary.append(t[0])
        print(t[0])
        print(" ")

"""### 4.2.2 Representación Frecuencia TF IDF"""

# SVD modelo y transformación del autor 1 representación frecuencias
start = datetime.now()
svdED_tfidf = TruncatedSVD(n_components=n_topics, random_state=64)
svdED_tfidf.fit_transform(dfED_tfidf)
end = datetime.now()
record = ["SVD TF-IDF A2: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecED.get_feature_names()
topEDSVDtfidf = []
for i, comp in enumerate(svdED_tfidf.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topEDSVDtfidf.append(t[0])
        print(t[0])
        print(" ")

"""## 4.3 Author 3

### 4.3.1 Representación Binaria
"""

# SVD modelo y transformación del autor 3 representación Binária
start = datetime.now()
svdVM_binary = TruncatedSVD(n_components=n_topics, random_state=64)
svdVM_binary.fit_transform(dfVM_binary)
end = datetime.now()
record = ["SVD Binary A3: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecVM.get_feature_names()
topVMSVDBinary = []
for i, comp in enumerate(svdVM_binary.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topVMSVDBinary.append(t[0])
        print(t[0])
        print(" ")

"""### 4.3.2 Representación Frecuencia TF IDF"""

# SVD modelo y transformación del autor 3 representación frecuencias
start = datetime.now()
svdVM_tfidf = TruncatedSVD(n_components=n_topics, random_state=64)
svdVM_tfidf.fit_transform(dfVM_tfidf)
end = datetime.now()
record = ["SVD TF-IDF A3: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecVM.get_feature_names()
topVMSVDtfidf = []
for i, comp in enumerate(svdVM_tfidf.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topVMSVDtfidf.append(t[0])
        print(t[0])
        print(" ")

"""## 4.4 Author 4

### 4.4.1 Representación Binaria
"""

# SVD modelo y transformación del autor 4 representación Binária
start = datetime.now()
svdIG_binary = TruncatedSVD(n_components=n_topics, random_state=64)
svdIG_binary.fit_transform(dfIG_binary)
end = datetime.now()
record = ["SVD Binary A4: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecIG.get_feature_names()
topIGSVDBinary = []
for i, comp in enumerate(svdIG_binary.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topIGSVDBinary.append(t[0])
        print(t[0])
        print(" ")

"""### 4.4.2 Representación Frecuencia TF IDF"""

# SVD modelo y transformación del autor 4 representación frecuencias
start = datetime.now()
svdIG_tfidf = TruncatedSVD(n_components=n_topics, random_state=64)
svdIG_tfidf.fit_transform(dfIG_tfidf)
end = datetime.now()
record = ["SVD TF-IDF A4: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecIG.get_feature_names()
topIGSVDtfidf = []
for i, comp in enumerate(svdIG_tfidf.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topIGSVDtfidf.append(t[0])
        print(t[0])
        print(" ")

"""#5. **pLSA Probabilistic Latent Semantic Analysis**"""

#Definición de pipeline para pLSA
#Este paso aplica para todos
pipelineAll = Pipeline(*DEFAULT_PIPELINE)
pipelineAll

"""## 5.1 *Autor 1*"""

#Creación de corpus para el primer autor
corpusDCplsa = Corpus(dfDC['text_without_stopwords'],pipelineAll)

#Modelo pLSA primer autor
plsaDC = PLSA(corpusDCplsa, n_topics, True)

#Encontrar el mejor modelo entre 3 iteraciones
start = datetime.now()
plsaResultDC = plsaDC.best_of(3)
end = datetime.now()
record = ["pLAS A1: %s" % (end - start)]
tiempos.append(record)

#Ver resultados pLSA
topDCPLSA = []
for t in range(n_topics):
  print("Topic #",t,":")
  for w in range(n_words):
    print(plsaResultDC.word_given_topic[:][t][w][0])
    topDCPLSA.append(plsaResultDC.word_given_topic[:][t][w][0])

"""## 5.2 Autor 2"""

#Creación de corpus para el segundo autor
corpusEDplsa = Corpus(dfED['text_without_stopwords'],pipelineAll)

#Modelo pLSA primer autor
plsaED = PLSA(corpusEDplsa, n_topics, True)

#Encontrar el mejor modelo entre 3 iteraciones
start = datetime.now()
plsaResultED = plsaED.best_of(3)
end = datetime.now()
record = ["pLAS A2: %s" % (end - start)]
tiempos.append(record)

#Ver resultados pLSA
topEDPLSA = []
for t in range(n_topics):
  print("Topic #",t,":")
  for w in range(n_words):
    print(plsaResultED.word_given_topic[:][t][w][0])
    topEDPLSA.append(plsaResultED.word_given_topic[:][t][w][0])

"""## 5.3 Autor 3"""

#Creación de corpus para el tercer autor
corpusVMplsa = Corpus(dfVM['text_without_stopwords'],pipelineAll)

#Modelo pLSA primer autor
plsaVM = PLSA(corpusVMplsa, n_topics, True)

#Encontrar el mejor modelo entre 3 iteraciones
start = datetime.now()
plsaResultVM = plsaVM.best_of(3)
end = datetime.now()
record = ["pLAS A3: %s" % (end - start)]
tiempos.append(record)

#Ver resultados pLSA
topVMPLSA = []
for t in range(n_topics):
  print("Topic #",t,":")
  for w in range(n_words):
    print(plsaResultVM.word_given_topic[:][t][w][0])
    topVMPLSA.append(plsaResultVM.word_given_topic[:][t][w][0])

"""## 5.4 Autor 4"""

#Creación de corpus para el cuarto autor
corpusIGplsa = Corpus(dfIG['text_without_stopwords'],pipelineAll)

#Modelo pLSA primer autor
plsaIG = PLSA(corpusIGplsa, n_topics, True)

#Encontrar el mejor modelo entre 3 iteraciones
start = datetime.now()
plsaResultIG = plsaIG.best_of(3)
end = datetime.now()
record = ["pLAS A4: %s" % (end - start)]
tiempos.append(record)

#Ver resultados pLSA
topIGPLSA = []
for t in range(n_topics):
  print("Topic #",t,":")
  for w in range(n_words):
    print(plsaResultIG.word_given_topic[:][t][w][0])
    topIGPLSA.append(plsaResultIG.word_given_topic[:][t][w][0])

"""#6. **LDA Latent Dirilchelt Allocation**

## 6.1 *Autor 1*

### 6.1.1 Representación Binária
"""

#Crear modelo LDA Autor 1
ldaDC_binary = LDA(n_components= n_topics, n_jobs=-1)

#Construir modelo
start = datetime.now()
ldaDC_binary.fit(dfDC_binary)
end = datetime.now()
record = ["LDA Binary A1: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecDC.get_feature_names()
topDCLDABinary = []
for i, comp in enumerate(ldaDC_binary.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topDCLDABinary.append(t[0])
        print(t[0])
        print(" ")

"""### 6.1.2 Representación Frecuencia TF IDF"""

#Crear modelo LDA Autor 1
ldaDC_tfidf = LDA(n_components= n_topics, n_jobs=-1)

#Construir modelo
start = datetime.now()
ldaDC_tfidf.fit(dfDC_tfidf)
end = datetime.now()
record = ["LDA TF-IDF A1: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecDC.get_feature_names()
topDCLDAtfidf = []
for i, comp in enumerate(ldaDC_tfidf.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topDCLDAtfidf.append(t[0])
        print(t[0])
        print(" ")

"""## 6.2 Autor 2

### 6.2.1 Representación Binária
"""

#Crear modelo LDA Autor 1
ldaED_binary = LDA(n_components= n_topics, n_jobs=-1)

#Construir modelo
start = datetime.now()
ldaED_binary.fit(dfED_binary)
end = datetime.now()
record = ["LDA Binary A2: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecED.get_feature_names()
topEDLDABinary = []
for i, comp in enumerate(ldaED_binary.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topEDLDABinary.append(t[0])
        print(t[0])
        print(" ")

"""### 6.2.2 Representación Frecuencia TF IDF"""

#Crear modelo LDA Autor 1
ldaED_tfidf = LDA(n_components= n_topics, n_jobs=-1)

#Construir modelo
start = datetime.now()
ldaED_tfidf.fit(dfED_tfidf)
end = datetime.now()
record = ["LDA TF-IDF A2: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecED.get_feature_names()
topEDLDAtfidf = []
for i, comp in enumerate(ldaED_tfidf.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topEDLDAtfidf.append(t[0])
        print(t[0])
        print(" ")

"""## 6.3 Autor 3

### 6.3.1 Representación Binária
"""

#Crear modelo LDA Autor 1
ldaVM_binary = LDA(n_components= n_topics, n_jobs=-1)

#Construir modelo
start = datetime.now()
ldaVM_binary.fit(dfVM_binary)
end = datetime.now()
record = ["LDA Binary A3: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecVM.get_feature_names()
topVMLDABinary = []
for i, comp in enumerate(ldaVM_binary.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topVMLDABinary.append(t[0])
        print(t[0])
        print(" ")

"""### 6.3.2 Representación Frecuencia TF IDF"""

#Crear modelo LDA Autor 1
ldaVM_tfidf = LDA(n_components= n_topics, n_jobs=-1)

#Construir modelo
start = datetime.now()
ldaVM_tfidf.fit(dfVM_tfidf)
end = datetime.now()
record = ["LDA TF-IDF A3: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecVM.get_feature_names()
topVMLDAtfidf = []
for i, comp in enumerate(ldaVM_tfidf.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topVMLDAtfidf.append(t[0])
        print(t[0])
        print(" ")

"""## 6.4 Autor 4

### 6.4.1 Representación Binária
"""

#Crear modelo LDA Autor 1
ldaIG_binary = LDA(n_components= n_topics, n_jobs=-1)

#Construir modelo
start = datetime.now()
ldaIG_binary.fit(dfIG_binary)
end = datetime.now()
record = ["LDA Binary A4: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecIG.get_feature_names()
topIGLDABinary = []
for i, comp in enumerate(ldaIG_binary.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topIGLDABinary.append(t[0])
        print(t[0])
        print(" ")

"""### 6.4.2 Representación Frecuencia TF IDF"""

#Crear modelo LDA Autor 1
ldaIG_tfidf = LDA(n_components= n_topics, n_jobs=-1)

#Construir modelo
start = datetime.now()
ldaIG_tfidf.fit(dfIG_tfidf)
end = datetime.now()
record = ["LDA TF-IDF A4: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecIG.get_feature_names()
topIGLDAtfidf = []
for i, comp in enumerate(ldaIG_tfidf.components_):
    terms_comp = zip(terms1, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
    print("Topic "+str(i)+": ")
    for t in sorted_terms:
        topIGLDAtfidf.append(t[0])
        print(t[0])
        print(" ")

"""# 7. **Neural Networks Autoencoders**

## 7.1 **Autor 1**

### 7.1.1 **TF IDF Autoencoder**
"""

#Longitud de palabras
maxlenDC_tfidf = dfDC_tfidf.shape[1]

# Modelo autoencoder 3 capas (Encoder-Lantent-Decoder)
encoder_inputs = keras.Input(shape=(maxlenDC_tfidf,))
encoded = Dense(n_topics, activation='linear', name='Topic-Modelling')(encoder_inputs)
decoded = Dense(maxlenDC_tfidf, activation='sigmoid', use_bias = True)(encoded)
autoencoderDC_tfidf = Model(encoder_inputs, decoded)
autoencoderDC_tfidf.compile(optimizer='adam', loss='mse')

# Train autoencoder for ## epochs
start = datetime.now()
autoencoderDC_tfidf.fit(dfDC_tfidf, dfDC_tfidf, batch_size=64, epochs=10)
end = datetime.now()
record = ["AE TF-IDF A1: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
topicsAutoencoder = np.array(autoencoderDC_tfidf.layers[2].get_weights()[0])
terms1 = countVecDC.get_feature_names()
topDCAEtfidf = []
for i, comp in enumerate(topicsAutoencoder):
    if i < n_topics:
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topDCAEtfidf.append(t[0])
          print(t[0])
          print(" ")

"""### 7.1.2 **Binary Autoencoder**"""

#Longitud de palabras
maxlenDC_binary = dfDC_binary.shape[1]

# Modelo autoencoder 3 capas (Encoder-Lantent-Decoder)
encoder_inputs = keras.Input(shape=(maxlenDC_binary,), name='encoder')
encoded = Dense(n_topics, activation='linear', name='latent')(encoder_inputs)
decoded = Dense(maxlenDC_binary, activation='sigmoid', use_bias = True, name='decoder')(encoded)
autoencoderDC_binary = Model(encoder_inputs, decoded)
autoencoderDC_binary.compile(optimizer='adam', loss='binary_crossentropy')

# Train autoencoder for ## epochs
start = datetime.now()
autoencoderDC_binary.fit(dfDC_binary, dfDC_binary,batch_size=64,epochs=21)
end = datetime.now()
record = ["AE Binary A1: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
topicsAutoencoder = np.array(autoencoderDC_binary.layers[2].get_weights()[0])
terms1 = countVecDC.get_feature_names()
topDCAEBinary = []
for i, comp in enumerate(topicsAutoencoder):
    if i < n_topics:
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topDCAEBinary.append(t[0])
          print(t[0])
          print(" ")

"""## 7.2 **Autor 2**

### 7.2.1 **TF IDF Autoencoder**
"""

#Longitud de palabras
maxlenED_tfidf = dfED_tfidf.shape[1]

# Modelo autoencoder 3 capas (Encoder-Lantent-Decoder)
encoder_inputs = keras.Input(shape=(maxlenED_tfidf,))
encoded = Dense(n_topics, activation='linear', name='Topic-Modelling')(encoder_inputs)
decoded = Dense(maxlenED_tfidf, activation='sigmoid', use_bias = True)(encoded)
autoencoderED_tfidf = Model(encoder_inputs, decoded)
autoencoderED_tfidf.compile(optimizer='adam', loss='mse')

# Train autoencoder for ## epochs
start = datetime.now()
autoencoderED_tfidf.fit(dfED_tfidf, dfED_tfidf, batch_size=64, epochs=7)
end = datetime.now()
record = ["AE TF-IDF A2: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
topicsAutoencoder = np.array(autoencoderED_tfidf.layers[2].get_weights()[0])
terms1 = countVecED.get_feature_names()
topEDAEtfidf = []
for i, comp in enumerate(topicsAutoencoder):
    if i < n_topics:
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topEDAEtfidf.append(t[0])
          print(t[0])
          print(" ")

"""### 7.2.2 **Binary Autoencoder**"""

#Longitud de palabras
maxlenED_binary = dfED_binary.shape[1]

# Modelo autoencoder 3 capas (Encoder-Lantent-Decoder)
encoder_inputs = keras.Input(shape=(maxlenED_binary,), name='encoder')
encoded = Dense(n_topics, activation='linear', name='latent')(encoder_inputs)
decoded = Dense(maxlenED_binary, activation='sigmoid', use_bias = True, name='decoder')(encoded)
autoencoderED_binary = Model(encoder_inputs, decoded)
autoencoderED_binary.compile(optimizer='adam', loss='binary_crossentropy')

# Train autoencoder for ## epochs
start = datetime.now()
autoencoderED_binary.fit(dfED_binary, dfED_binary,batch_size=64,epochs=15)
end = datetime.now()
record = ["AE Binary A2: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
topicsAutoencoder = np.array(autoencoderED_binary.layers[2].get_weights()[0])
terms1 = countVecED.get_feature_names()
topEDAEBinary = []
for i, comp in enumerate(topicsAutoencoder):
    if i < n_topics:
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topEDAEBinary.append(t[0])
          print(t[0])
          print(" ")

"""## 7.3 **Autor 3**

### 7.3.1 **TF IDF Autoencoder**
"""

#Longitud de palabras
maxlenVM_tfidf = dfVM_tfidf.shape[1]

# Modelo autoencoder 3 capas (Encoder-Lantent-Decoder)
encoder_inputs = keras.Input(shape=(maxlenVM_tfidf,))
encoded = Dense(n_topics, activation='linear', name='Topic-Modelling')(encoder_inputs)
decoded = Dense(maxlenVM_tfidf, activation='sigmoid', use_bias = True)(encoded)
autoencoderVM_tfidf = Model(encoder_inputs, decoded)
autoencoderVM_tfidf.compile(optimizer='adam', loss='mse')

# Train autoencoder for ## epochs
start = datetime.now()
autoencoderVM_tfidf.fit(dfVM_tfidf, dfVM_tfidf, batch_size=64, epochs=4)
end = datetime.now()
record = ["AE TF-IDF A3: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
topicsAutoencoder = np.array(autoencoderVM_tfidf.layers[2].get_weights()[0])
terms1 = countVecVM.get_feature_names()
topVMAEtfidf = []
for i, comp in enumerate(topicsAutoencoder):
    if i < n_topics:
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topVMAEtfidf.append(t[0])
          print(t[0])
          print(" ")

"""### 7.3.2 **Binary Autoencoder**"""

#Longitud de palabras
maxlenVM_binary = dfVM_binary.shape[1]

# Modelo autoencoder 3 capas (Encoder-Lantent-Decoder)
encoder_inputs = keras.Input(shape=(maxlenVM_binary,), name='encoder')
encoded = Dense(n_topics, activation='linear', name='latent')(encoder_inputs)
decoded = Dense(maxlenVM_binary, activation='sigmoid', use_bias = True, name='decoder')(encoded)
autoencoderVM_binary = Model(encoder_inputs, decoded)
autoencoderVM_binary.compile(optimizer='adam', loss='binary_crossentropy')

# Train autoencoder for ## epochs
start = datetime.now()
autoencoderVM_binary.fit(dfVM_binary, dfVM_binary,batch_size=64,epochs=7)
end = datetime.now()
record = ["AE Binary A3: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
topicsAutoencoder = np.array(autoencoderVM_binary.layers[2].get_weights()[0])
terms1 = countVecVM.get_feature_names()
topVMAEBinary = []
for i, comp in enumerate(topicsAutoencoder):
    if i < n_topics:
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topVMAEBinary.append(t[0])
          print(t[0])
          print(" ")

"""## 7.4 **Autor 4**

### 7.4.1 **TF IDF Autoencoder**
"""

#Longitud de palabras
maxlenIG_tfidf = dfIG_tfidf.shape[1]

# Modelo autoencoder 3 capas (Encoder-Lantent-Decoder)
encoder_inputs = keras.Input(shape=(maxlenIG_tfidf,))
encoded = Dense(n_topics, activation='linear', name='Topic-Modelling')(encoder_inputs)
decoded = Dense(maxlenIG_tfidf, activation='sigmoid', use_bias = True)(encoded)
autoencoderIG_tfidf = Model(encoder_inputs, decoded)
autoencoderIG_tfidf.compile(optimizer='adam', loss='mse')

# Train autoencoder for ## epochs
start = datetime.now()
autoencoderIG_tfidf.fit(dfIG_tfidf, dfIG_tfidf, batch_size=64, epochs=4)
end = datetime.now()
record = ["AE TF-IDF A4: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
topicsAutoencoder = np.array(autoencoderIG_tfidf.layers[2].get_weights()[0])
terms1 = countVecIG.get_feature_names()
topIGAEtfidf = []
for i, comp in enumerate(topicsAutoencoder):
    if i < n_topics:
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topIGAEtfidf.append(t[0])
          print(t[0])
          print(" ")

"""### 7.4.2 **Binary Autoencoder**"""

#Longitud de palabras
maxlenIG_binary = dfIG_binary.shape[1]

# Modelo autoencoder 3 capas (Encoder-Lantent-Decoder)
encoder_inputs = keras.Input(shape=(maxlenIG_binary,), name='encoder')
encoded = Dense(n_topics, activation='linear', name='latent')(encoder_inputs)
decoded = Dense(maxlenIG_binary, activation='sigmoid', use_bias = True, name='decoder')(encoded)
autoencoderIG_binary = Model(encoder_inputs, decoded)
autoencoderIG_binary.compile(optimizer='adam', loss='binary_crossentropy')

# Train autoencoder for ## epochs
start = datetime.now()
autoencoderIG_binary.fit(dfIG_binary, dfIG_binary,batch_size=64,epochs=8)
end = datetime.now()
record = ["AE Binary A4: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
topicsAutoencoder = np.array(autoencoderIG_binary.layers[2].get_weights()[0])
terms1 = countVecIG.get_feature_names()
topIGAEBinary = []
for i, comp in enumerate(topicsAutoencoder):
    if i < n_topics:
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topIGAEBinary.append(t[0])
          print(t[0])
          print(" ")

"""# 8. **Neural Networks Maquinas de Boltzmann**

## 8.1 **Autor 1**

### 8.1.1 **Binary RBM**
"""

#Construcción Modelo
rbmDC_binary = BernoulliRBM(n_components=n_topics, learning_rate=0.01, batch_size=64, random_state=42, verbose=True)
#Entrenamiento
start = datetime.now()
rbmDC_binary.fit_transform(dfDC_binary)
end = datetime.now()
record = ["RBM Binary A1: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecDC.get_feature_names()
topDCRBMBinary = []
for i, comp in enumerate(rbmDC_binary.components_):
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topDCRBMBinary.append(t[0])
          print(t[0])
          print(" ")

"""### 8.1.2 **TF IDF RBM**"""

#Construcción Modelo
rbmDC_tfidf = BernoulliRBM(n_components=n_topics, learning_rate=0.01, batch_size=64, random_state=42, verbose=True)
#Entrenamiento
start = datetime.now()
rbmDC_tfidf.fit_transform(dfDC_tfidf)
end = datetime.now()
record = ["RBM TF-IDF A1: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecDC.get_feature_names()
topDCRBMtfidf = []
for i, comp in enumerate(rbmDC_tfidf.components_):
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topDCRBMtfidf.append(t[0])
          print(t[0])
          print(" ")

"""## 8.2 **Autor 2**

### 8.2.1 **Binary RBM**
"""

#Construcción Modelo
rbmED_binary = BernoulliRBM(n_components=n_topics, learning_rate=0.01, batch_size=64, random_state=42, verbose=True)
#Entrenamiento
start = datetime.now()
rbmED_binary.fit_transform(dfED_binary)
end = datetime.now()
record = ["RBM Binary A2: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecED.get_feature_names()
topEDRBMBinary = []
for i, comp in enumerate(rbmED_binary.components_):
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topEDRBMBinary.append(t[0])
          print(t[0])
          print(" ")

"""### 8.2.2 **TF IDF RBM**"""

#Construcción Modelo
rbmED_tfidf = BernoulliRBM(n_components=n_topics, learning_rate=0.02, batch_size=64, random_state=42, verbose=True)
#Entrenamiento
start = datetime.now()
rbmED_tfidf.fit_transform(dfED_tfidf)
end = datetime.now()
record = ["RBM TF-IDF A2: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecED.get_feature_names()
topEDRBMtfidf = []
for i, comp in enumerate(rbmED_tfidf.components_):
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topEDRBMtfidf.append(t[0])
          print(t[0])
          print(" ")

"""## 8.3 **Autor 3**

### 8.3.1 **Binary RBM**
"""

#Construcción Modelo
rbmVM_binary = BernoulliRBM(n_components=n_topics, learning_rate=0.01, batch_size=64, random_state=42, verbose=True)
#Entrenamiento
start = datetime.now()
rbmVM_binary.fit_transform(dfVM_binary)
end = datetime.now()
record = ["RBM Binary A3: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecVM.get_feature_names()
topVMRBMBinary = []
for i, comp in enumerate(rbmVM_binary.components_):
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topVMRBMBinary.append(t[0])
          print(t[0])
          print(" ")

"""### 8.3.2 **TF IDF RBM**"""

#Construcción Modelo
rbmVM_tfidf = BernoulliRBM(n_components=n_topics, learning_rate=0.01, batch_size=64, random_state=42, verbose=True)
#Entrenamiento
start = datetime.now()
rbmVM_tfidf.fit_transform(dfVM_tfidf)
end = datetime.now()
record = ["RBM TF-IDF A3: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecVM.get_feature_names()
topVMRBMtfidf = []
for i, comp in enumerate(rbmVM_tfidf.components_):
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topVMRBMtfidf.append(t[0])
          print(t[0])
          print(" ")

"""## 8.4 **Autor 4**

### 8.4.1 **Binary RBM**
"""

#Construcción Modelo
rbmIG_binary = BernoulliRBM(n_components=n_topics, learning_rate=0.02, batch_size=64, random_state=42, verbose=True)
#Entrenamiento
start = datetime.now()
rbmIG_binary.fit_transform(dfIG_binary)
end = datetime.now()
record = ["RBM Binary A4: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecIG.get_feature_names()
topIGRBMBinary = []
for i, comp in enumerate(rbmIG_binary.components_):
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topIGRBMBinary.append(t[0])
          print(t[0])
          print(" ")

"""### 8.4.2 **TF IDF RBM**"""

#Construcción Modelo
rbmIG_tfidf = BernoulliRBM(n_components=n_topics, learning_rate=0.01, batch_size=64, random_state=42, verbose=True)
#Entrenamiento
start = datetime.now()
rbmIG_tfidf.fit_transform(dfIG_tfidf)
end = datetime.now()
record = ["RBM TF-IDF A4: %s" % (end - start)]
tiempos.append(record)

#Visualizar los tópicos
terms1 = countVecIG.get_feature_names()
topIGRBMtfidf = []
for i, comp in enumerate(rbmIG_tfidf.components_):
      terms_comp = zip(terms1, comp)
      sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:n_words]
      print("Topic "+str(i)+": ")
      for t in sorted_terms:
          topIGRBMtfidf.append(t[0])
          print(t[0])
          print(" ")

"""#9. **Resultados de experimento (Métricas)**

## 9.1 Gráficas experimento

### 9.2.1 Organización de tópicos
"""

#Organizar listas de tópicos
#Autor 1
#SVD
topicsDCSVDBinary = chunkIt(topDCSVDBinary,n_topics)
topicsDCSVDtfidf = chunkIt(topDCSVDtfidf,n_topics)
#pLSA
topicsDCPLSA = chunkIt(topDCPLSA,n_topics)
#LDA
topicsDCLDABinary = chunkIt(topDCLDABinary,n_topics)
topicsDCLDAtfidf = chunkIt(topDCLDAtfidf,n_topics)
#autoencoder
topicsDCAEBinary = chunkIt(topDCAEBinary,n_topics)
topicsDCAEtfidf = chunkIt(topDCAEtfidf,n_topics)
#rbm
topicsDCRBMBinary = chunkIt(topDCRBMBinary,n_topics)
topicsDCRBMtfidf = chunkIt(topDCRBMtfidf,n_topics)

#Organizar listas de tópicos
#Autor 2
try:
 topEDPLSA[topEDPLSA.index('machine')] = 'maquina'
 topEDPLSA[topEDPLSA.index('team')] = 'equipo'
except:
  None
#SVD
topicsEDSVDBinary = chunkIt(topEDSVDBinary,n_topics)
topicsEDSVDtfidf = chunkIt(topEDSVDtfidf,n_topics)
#pLSA
topicsEDPLSA = chunkIt(topEDPLSA,n_topics)
#LDA
topicsEDLDABinary = chunkIt(topEDLDABinary,n_topics)
topicsEDLDAtfidf = chunkIt(topEDLDAtfidf,n_topics)
#autoencoder
topicsEDAEBinary = chunkIt(topEDAEBinary,n_topics)
topicsEDAEtfidf = chunkIt(topEDAEtfidf,n_topics)
#rbm
topicsEDRBMBinary = chunkIt(topEDRBMBinary,n_topics)
topicsEDRBMtfidf = chunkIt(topEDRBMtfidf,n_topics)

#Organizar listas de tópicos
#Autor 3
try:
 topVMPLSA[topVMPLSA.index('multiple')] = 'varios'
 topVMPLSA[topVMPLSA.index('persona')] = 'ser'
except:
  None
#SVD
topicsVMSVDBinary = chunkIt(topVMSVDBinary,n_topics)
topicsVMSVDtfidf = chunkIt(topVMSVDtfidf,n_topics)
#pLSA
topicsVMPLSA = chunkIt(topVMPLSA,n_topics)
#LDA
topicsVMLDABinary = chunkIt(topVMLDABinary,n_topics)
topicsVMLDAtfidf = chunkIt(topVMLDAtfidf,n_topics)
#autoencoder
topicsVMAEBinary = chunkIt(topVMAEBinary,n_topics)
topicsVMAEtfidf = chunkIt(topVMAEtfidf,n_topics)
#rbm
topicsVMRBMBinary = chunkIt(topVMRBMBinary,n_topics)
topicsVMRBMtfidf = chunkIt(topVMRBMtfidf,n_topics)

#Organizar listas de tópicos
#Autor 4
#SVD
topicsIGSVDBinary = chunkIt(topIGSVDBinary,n_topics)
topicsIGSVDtfidf = chunkIt(topIGSVDtfidf,n_topics)
#pLSA
topicsIGPLSA = chunkIt(topIGPLSA,n_topics)
#LDA
topicsIGLDABinary = chunkIt(topIGLDABinary,n_topics)
topicsIGLDAtfidf = chunkIt(topIGLDAtfidf,n_topics)
#autoencoder
topicsIGAEBinary = chunkIt(topIGAEBinary,n_topics)
topicsIGAEtfidf = chunkIt(topIGAEtfidf,n_topics)
#rbm
topicsIGRBMBinary = chunkIt(topIGRBMBinary,n_topics)
topicsIGRBMtfidf = chunkIt(topIGRBMtfidf,n_topics)

doc_lens = [len(d) for d in dfDC['texto_procesado']]

# Plot
plt.figure(figsize=(16,7), dpi=160)
plt.hist(doc_lens, bins = 1000, color='navy')
plt.text(750, 100, "Mean   : " + str(round(np.mean(doc_lens))))
plt.text(750,  90, "Median : " + str(round(np.median(doc_lens))))
plt.text(750,  80, "Stdev   : " + str(round(np.std(doc_lens))))
plt.text(750,  70, "1%ile    : " + str(round(np.quantile(doc_lens, q=0.01))))
plt.text(750,  60, "99%ile  : " + str(round(np.quantile(doc_lens, q=0.99))))

plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')
plt.tick_params(size=16)
plt.xticks(np.linspace(0,1000,9))
plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))
plt.show()

"""### 9.1.2 Visualización de tópicos"""

#binary

#lda binary Autor1
#visDCLDABinary = skpyLDAvis.prepare(ldaDC_binary, countDC, countVecDC)
#pyLDAvis.save_html(visDCLDABinary, '/content/drive/MyDrive/TrainData/authors/visDCLDABinary.html')
#lda binary Autor2
#visEDLDABinary = skpyLDAvis.prepare(ldaED_binary, countED, countVecED)
#pyLDAvis.save_html(visEDLDABinary, '/content/drive/MyDrive/TrainData/authors/visEDLDABinary.html')
#lda binary Autor3
#visVMLDABinary = skpyLDAvis.prepare(ldaVM_binary, countVM, countVecVM)
#pyLDAvis.save_html(visVMLDABinary, '/content/drive/MyDrive/TrainData/authors/visVMLDABinary.html')
#lda binary Autor4
#visIGLDABinary = skpyLDAvis.prepare(ldaIG_binary, countIG, countVecIG)
#pyLDAvis.save_html(visIGLDABinary, '/content/drive/MyDrive/TrainData/authors/visIGLDABinary.html')

#TFIDF

#lda tfidf Autor1
#visDCLDAtfidf = skpyLDAvis.prepare(ldaDC_tfidf, countDC, countVecDC)
#pyLDAvis.save_html(visDCLDAtfidf, '/content/drive/MyDrive/TrainData/authors/visDCLDAtfidf.html')
#lda tfidf Autor2
#visEDLDAtfidf = skpyLDAvis.prepare(ldaED_tfidf, countED, countVecED)
#pyLDAvis.save_html(visEDLDAtfidf, '/content/drive/MyDrive/TrainData/authors/visEDLDAtfidf.html')
#lda tfidf Autor3
#visVMLDAtfidf = skpyLDAvis.prepare(ldaVM_tfidf, countVM, countVecVM)
#pyLDAvis.save_html(visVMLDAtfidf, '/content/drive/MyDrive/TrainData/authors/visVMLDAtfidf.html')
#lda tfidf Autor4
#visIGLDAtfidf = skpyLDAvis.prepare(ldaIG_tfidf, countIG, countVecIG)
#pyLDAvis.save_html(visIGLDAtfidf, '/content/drive/MyDrive/TrainData/authors/visIGLDAtfidf.html')

"""### 9.1.1 pLSA Nube de palabras"""

#Análisis de nube de palabras
visualize = Visualize(plsaResultDC)
fig = plt.figure(figsize=(9.4, 10))
_ = visualize.wordclouds(fig)

# Crear una cadena de string con todos los tweets
text = ','.join(list(dfDC['texto_procesado'].values))
tokens_speech = [word for word in word_tokenize(text)]
long_string = ','.join(list(tokens_speech))
# Crear un objeto WordCloud  
wordcloud = WordCloud(background_color="white", contour_width=3, contour_color='steelblue', stopwords=spanish_stopwords)
# Generar la nube de palabras
wordcloud.generate(long_string)
# Visualizar
wordcloud.to_image()

# Crear una cadena de string con todos los tweets
from wordcloud import WordCloud
text = ','.join(list(dfED['texto_procesado'].values))
tokens_speech = [word for word in word_tokenize(text)]
long_string = ','.join(list(tokens_speech))
# Crear un objeto WordCloud  
wordcloud = WordCloud(background_color="white", contour_width=3, contour_color='steelblue', stopwords=spanish_stopwords)
# Generar la nube de palabras
wordcloud.generate(long_string)
# Visualizar
wordcloud.to_image()

# Crear una cadena de string con todos los tweets
from wordcloud import WordCloud
text = ','.join(list(dfVM['texto_procesado'].values))
tokens_speech = [word for word in word_tokenize(text)]
long_string = ','.join(list(tokens_speech))
# Crear un objeto WordCloud  
wordcloud = WordCloud(background_color="white", contour_width=3, contour_color='steelblue', stopwords=spanish_stopwords)
# Generar la nube de palabras
wordcloud.generate(long_string)
# Visualizar
wordcloud.to_image()

# Crear una cadena de string con todos los tweets
from wordcloud import WordCloud
text = ','.join(list(dfIG['texto_procesado'].values))
tokens_speech = [word for word in word_tokenize(text)]
long_string = ','.join(list(tokens_speech))
# Crear un objeto WordCloud  
wordcloud = WordCloud(background_color="white", contour_width=3, contour_color='steelblue', stopwords=spanish_stopwords)
# Generar la nube de palabras
wordcloud.generate(long_string)
# Visualizar
wordcloud.to_image()

"""## 9.2 Análisis de coherencia

### 9.2.2 Autor 1
"""

#U_MASS
#Analisis de coherencia SVD
#Binary
cmDCSVDBinary = CoherenceModel(topics=topicsDCSVDBinary, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCSVDBinary = cmDCSVDBinary.get_coherence()  # get coherence value
#TF IDF
cmDCSVDtfidf = CoherenceModel(topics=topicsDCSVDtfidf, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCSVDtfidf = cmDCSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
cmDCPLSA = CoherenceModel(topics=topicsDCPLSA, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCPLSA = cmDCPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
cmDCLDABinary = CoherenceModel(topics=topicsDCLDABinary, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCLDABinary = cmDCLDABinary.get_coherence()  # get coherence value
#TF IDF
cmDCLDAtfidf = CoherenceModel(topics=topicsDCLDAtfidf, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCLDAtfidf = cmDCLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
cmDCAEBinary = CoherenceModel(topics=topicsDCAEBinary, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCAEBinary = cmDCAEBinary.get_coherence()  # get coherence value
#TF IDF
cmDCAEtfidf = CoherenceModel(topics=topicsDCAEtfidf, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCAEtfidf = cmDCAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
cmDCRBMBinary = CoherenceModel(topics=topicsDCRBMBinary, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCRBMBinary = cmDCRBMBinary.get_coherence()  # get coherence value
#TF IDF
cmDCRBMtfidf = CoherenceModel(topics=topicsDCRBMtfidf, corpus=corpusDC, dictionary=dictDC, coherence='u_mass')
coherenceDCRBMtfidf = cmDCRBMtfidf.get_coherence()  # get coherence value

# C_V
#Analisis de coherencia SVD
#Binary
chcvDCSVDBinary = CoherenceModel(topics=topicsDCSVDBinary, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCSVDBinary = chcvDCSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcvDCSVDtfidf = CoherenceModel(topics=topicsDCSVDtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCSVDtfidf = chcvDCSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcvDCPLSA = CoherenceModel(topics=topicsDCPLSA, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCPLSA = chcvDCPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcvDCLDABinary = CoherenceModel(topics=topicsDCLDABinary, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCLDABinary = chcvDCLDABinary.get_coherence()  # get coherence value
#TF IDF
chcvDCLDAtfidf = CoherenceModel(topics=topicsDCLDAtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCLDAtfidf = chcvDCLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcvDCAEBinary = CoherenceModel(topics=topicsDCAEBinary, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCAEBinary = chcvDCAEBinary.get_coherence()  # get coherence value
#TF IDF
chcvDCAEtfidf = CoherenceModel(topics=topicsDCAEtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCAEtfidf = chcvDCAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcvDCRBMBinary = CoherenceModel(topics=topicsDCRBMBinary, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCRBMBinary = chcvDCRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcvDCRBMtfidf = CoherenceModel(topics=topicsDCRBMtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_v')
cvDCRBMtfidf = chcvDCRBMtfidf.get_coherence()  # get coherence value

# c_uci
#Analisis de coherencia SVD
#Binary
chcuciDCSVDBinary = CoherenceModel(topics=topicsDCSVDBinary, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCSVDBinary = chcuciDCSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcuciDCSVDtfidf = CoherenceModel(topics=topicsDCSVDtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCSVDtfidf = chcuciDCSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcuciDCPLSA = CoherenceModel(topics=topicsDCPLSA, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCPLSA = chcuciDCPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcuciDCLDABinary = CoherenceModel(topics=topicsDCLDABinary, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCLDABinary = chcuciDCLDABinary.get_coherence()  # get coherence value
#TF IDF
chcuciDCLDAtfidf = CoherenceModel(topics=topicsDCLDAtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCLDAtfidf = chcuciDCLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcuciDCAEBinary = CoherenceModel(topics=topicsDCAEBinary, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCAEBinary = chcuciDCAEBinary.get_coherence()  # get coherence value
#TF IDF
chcuciDCAEtfidf = CoherenceModel(topics=topicsDCAEtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCAEtfidf = chcuciDCAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcuciDCRBMBinary = CoherenceModel(topics=topicsDCRBMBinary, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCRBMBinary = chcuciDCRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcuciDCRBMtfidf = CoherenceModel(topics=topicsDCRBMtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_uci')
cuciDCRBMtfidf = chcuciDCRBMtfidf.get_coherence()  # get coherence value

# c_npmi
#Analisis de coherencia SVD
#Binary
chcnpmiDCSVDBinary = CoherenceModel(topics=topicsDCSVDBinary, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCSVDBinary = chcnpmiDCSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiDCSVDtfidf = CoherenceModel(topics=topicsDCSVDtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCSVDtfidf = chcnpmiDCSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcnpmiDCPLSA = CoherenceModel(topics=topicsDCPLSA, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCPLSA = chcnpmiDCPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcnpmiDCLDABinary = CoherenceModel(topics=topicsDCLDABinary, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCLDABinary = chcnpmiDCLDABinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiDCLDAtfidf = CoherenceModel(topics=topicsDCLDAtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCLDAtfidf = chcnpmiDCLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcnpmiDCAEBinary = CoherenceModel(topics=topicsDCAEBinary, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCAEBinary = chcnpmiDCAEBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiDCAEtfidf = CoherenceModel(topics=topicsDCAEtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCAEtfidf = chcnpmiDCAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcnpmiDCRBMBinary = CoherenceModel(topics=topicsDCRBMBinary, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCRBMBinary = chcnpmiDCRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiDCRBMtfidf = CoherenceModel(topics=topicsDCRBMtfidf, texts=tokenDC, dictionary=dictDC, coherence='c_npmi')
cnpmiDCRBMtfidf = chcnpmiDCRBMtfidf.get_coherence()  # get coherence value

"""### 9.2.3 Autor 2"""

#Analisis de coherencia SVD
#Binary
cmEDSVDBinary = CoherenceModel(topics=topicsEDSVDBinary, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDSVDBinary = cmEDSVDBinary.get_coherence()  # get coherence value
#TF IDF
cmEDSVDtfidf = CoherenceModel(topics=topicsEDSVDtfidf, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDSVDtfidf = cmEDSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
cmEDPLSA = CoherenceModel(topics=topicsEDPLSA, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDPLSA = cmEDPLSA.get_coherence()  # get coherence value
#Analisis de coherencia LDA
#Binary
cmEDLDABinary = CoherenceModel(topics=topicsEDLDABinary, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDLDABinary = cmEDLDABinary.get_coherence()  # get coherence value
#TF IDF
cmEDLDAtfidf = CoherenceModel(topics=topicsEDLDAtfidf, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDLDAtfidf = cmEDLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
cmEDAEBinary = CoherenceModel(topics=topicsEDAEBinary, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDAEBinary = cmEDAEBinary.get_coherence()  # get coherence value
#TF IDF
cmEDAEtfidf = CoherenceModel(topics=topicsEDAEtfidf, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDAEtfidf = cmEDAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
cmEDRBMBinary = CoherenceModel(topics=topicsEDRBMBinary, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDRBMBinary = cmEDRBMBinary.get_coherence()  # get coherence value
#TF IDF
cmEDRBMtfidf = CoherenceModel(topics=topicsEDRBMtfidf, corpus=corpusED, dictionary=dictED, coherence='u_mass')
coherenceEDRBMtfidf = cmEDRBMtfidf.get_coherence()  # get coherence value

# C_V
#Analisis de coherencia SVD
#Binary
chcvEDSVDBinary = CoherenceModel(topics=topicsEDSVDBinary, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDSVDBinary = chcvEDSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcvEDSVDtfidf = CoherenceModel(topics=topicsEDSVDtfidf, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDSVDtfidf = chcvEDSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcvEDPLSA = CoherenceModel(topics=topicsEDPLSA, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDPLSA = chcvEDPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcvEDLDABinary = CoherenceModel(topics=topicsEDLDABinary, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDLDABinary = chcvEDLDABinary.get_coherence()  # get coherence value
#TF IDF
chcvEDLDAtfidf = CoherenceModel(topics=topicsEDLDAtfidf, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDLDAtfidf = chcvEDLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcvEDAEBinary = CoherenceModel(topics=topicsEDAEBinary, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDAEBinary = chcvEDAEBinary.get_coherence()  # get coherence value
#TF IDF
chcvEDAEtfidf = CoherenceModel(topics=topicsEDAEtfidf, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDAEtfidf = chcvEDAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcvEDRBMBinary = CoherenceModel(topics=topicsEDRBMBinary, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDRBMBinary = chcvEDRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcvEDRBMtfidf = CoherenceModel(topics=topicsEDRBMtfidf, texts=tokenED, dictionary=dictED, coherence='c_v')
cvEDRBMtfidf = chcvEDRBMtfidf.get_coherence()  # get coherence value

# c_uci
#Analisis de coherencia SVD
#Binary
chcuciEDSVDBinary = CoherenceModel(topics=topicsEDSVDBinary, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDSVDBinary = chcuciEDSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcuciEDSVDtfidf = CoherenceModel(topics=topicsEDSVDtfidf, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDSVDtfidf = chcuciEDSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcuciEDPLSA = CoherenceModel(topics=topicsEDPLSA, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDPLSA = chcuciEDPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcuciEDLDABinary = CoherenceModel(topics=topicsEDLDABinary, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDLDABinary = chcuciEDLDABinary.get_coherence()  # get coherence value
#TF IDF
chcuciEDLDAtfidf = CoherenceModel(topics=topicsEDLDAtfidf, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDLDAtfidf = chcuciEDLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcuciEDAEBinary = CoherenceModel(topics=topicsEDAEBinary, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDAEBinary = chcuciEDAEBinary.get_coherence()  # get coherence value
#TF IDF
chcuciEDAEtfidf = CoherenceModel(topics=topicsEDAEtfidf, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDAEtfidf = chcuciEDAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcuciEDRBMBinary = CoherenceModel(topics=topicsEDRBMBinary, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDRBMBinary = chcuciEDRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcuciEDRBMtfidf = CoherenceModel(topics=topicsEDRBMtfidf, texts=tokenED, dictionary=dictED, coherence='c_uci')
cuciEDRBMtfidf = chcuciEDRBMtfidf.get_coherence()  # get coherence value

# c_npmi
#Analisis de coherencia SVD
#Binary
chcnpmiEDSVDBinary = CoherenceModel(topics=topicsEDSVDBinary, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDSVDBinary = chcnpmiEDSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiEDSVDtfidf = CoherenceModel(topics=topicsEDSVDtfidf, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDSVDtfidf = chcnpmiEDSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcnpmiEDPLSA = CoherenceModel(topics=topicsEDPLSA, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDPLSA = chcnpmiEDPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcnpmiEDLDABinary = CoherenceModel(topics=topicsEDLDABinary, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDLDABinary = chcnpmiEDLDABinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiEDLDAtfidf = CoherenceModel(topics=topicsEDLDAtfidf, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDLDAtfidf = chcnpmiEDLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcnpmiEDAEBinary = CoherenceModel(topics=topicsEDAEBinary, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDAEBinary = chcnpmiEDAEBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiEDAEtfidf = CoherenceModel(topics=topicsEDAEtfidf, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDAEtfidf = chcnpmiEDAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcnpmiEDRBMBinary = CoherenceModel(topics=topicsEDRBMBinary, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDRBMBinary = chcnpmiEDRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiEDRBMtfidf = CoherenceModel(topics=topicsEDRBMtfidf, texts=tokenED, dictionary=dictED, coherence='c_npmi')
cnpmiEDRBMtfidf = chcnpmiEDRBMtfidf.get_coherence()  # get coherence value

"""### 9.2.4 Autor 3"""

#Analisis de coherencia SVD
#Binary
cmVMSVDBinary = CoherenceModel(topics=topicsVMSVDBinary, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMSVDBinary = cmVMSVDBinary.get_coherence()  # get coherence value
#TF IDF
cmVMSVDtfidf = CoherenceModel(topics=topicsVMSVDtfidf, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMSVDtfidf = cmVMSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
cmVMPLSA = CoherenceModel(topics=topicsVMPLSA, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMPLSA = cmVMPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
cmVMLDABinary = CoherenceModel(topics=topicsVMLDABinary, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMLDABinary = cmVMLDABinary.get_coherence()  # get coherence value
#TF IDF
cmVMLDAtfidf = CoherenceModel(topics=topicsVMLDAtfidf, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMLDAtfidf = cmVMLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
cmVMAEBinary = CoherenceModel(topics=topicsVMAEBinary, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMAEBinary = cmVMAEBinary.get_coherence()  # get coherence value
#TF IDF
cmVMAEtfidf = CoherenceModel(topics=topicsVMAEtfidf, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMAEtfidf = cmVMAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
cmVMRBMBinary = CoherenceModel(topics=topicsVMRBMBinary, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMRBMBinary = cmVMRBMBinary.get_coherence()  # get coherence value
#TF IDF
cmVMRBMtfidf = CoherenceModel(topics=topicsVMRBMtfidf, corpus=corpusVM, dictionary=dictVM, coherence='u_mass')
coherenceVMRBMtfidf = cmVMRBMtfidf.get_coherence()  # get coherence value

# C_V
#Analisis de coherencia SVD
#Binary
chcvVMSVDBinary = CoherenceModel(topics=topicsVMSVDBinary, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMSVDBinary = chcvVMSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcvVMSVDtfidf = CoherenceModel(topics=topicsVMSVDtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMSVDtfidf = chcvVMSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcvVMPLSA = CoherenceModel(topics=topicsVMPLSA, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMPLSA = chcvVMPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcvVMLDABinary = CoherenceModel(topics=topicsVMLDABinary, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMLDABinary = chcvVMLDABinary.get_coherence()  # get coherence value
#TF IDF
chcvVMLDAtfidf = CoherenceModel(topics=topicsVMLDAtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMLDAtfidf = chcvVMLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcvVMAEBinary = CoherenceModel(topics=topicsVMAEBinary, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMAEBinary = chcvVMAEBinary.get_coherence()  # get coherence value
#TF IDF
chcvVMAEtfidf = CoherenceModel(topics=topicsVMAEtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMAEtfidf = chcvVMAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcvVMRBMBinary = CoherenceModel(topics=topicsVMRBMBinary, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMRBMBinary = chcvVMRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcvVMRBMtfidf = CoherenceModel(topics=topicsVMRBMtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_v')
cvVMRBMtfidf = chcvVMRBMtfidf.get_coherence()  # get coherence value

# c_uci
#Analisis de coherencia SVD
#Binary
chcuciVMSVDBinary = CoherenceModel(topics=topicsVMSVDBinary, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMSVDBinary = chcuciVMSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcuciVMSVDtfidf = CoherenceModel(topics=topicsVMSVDtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMSVDtfidf = chcuciVMSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcuciVMPLSA = CoherenceModel(topics=topicsVMPLSA, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMPLSA = chcuciVMPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcuciVMLDABinary = CoherenceModel(topics=topicsVMLDABinary, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMLDABinary = chcuciVMLDABinary.get_coherence()  # get coherence value
#TF IDF
chcuciVMLDAtfidf = CoherenceModel(topics=topicsVMLDAtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMLDAtfidf = chcuciVMLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcuciVMAEBinary = CoherenceModel(topics=topicsVMAEBinary, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMAEBinary = chcuciVMAEBinary.get_coherence()  # get coherence value
#TF IDF
chcuciVMAEtfidf = CoherenceModel(topics=topicsVMAEtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMAEtfidf = chcuciVMAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcuciVMRBMBinary = CoherenceModel(topics=topicsVMRBMBinary, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMRBMBinary = chcuciVMRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcuciVMRBMtfidf = CoherenceModel(topics=topicsVMRBMtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_uci')
cuciVMRBMtfidf = chcuciVMRBMtfidf.get_coherence()  # get coherence value

# c_npmi
#Analisis de coherencia SVD
#Binary
chcnpmiVMSVDBinary = CoherenceModel(topics=topicsVMSVDBinary, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMSVDBinary = chcnpmiVMSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiVMSVDtfidf = CoherenceModel(topics=topicsVMSVDtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMSVDtfidf = chcnpmiVMSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcnpmiVMPLSA = CoherenceModel(topics=topicsVMPLSA, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMPLSA = chcnpmiVMPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcnpmiVMLDABinary = CoherenceModel(topics=topicsVMLDABinary, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMLDABinary = chcnpmiVMLDABinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiVMLDAtfidf = CoherenceModel(topics=topicsVMLDAtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMLDAtfidf = chcnpmiVMLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcnpmiVMAEBinary = CoherenceModel(topics=topicsVMAEBinary, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMAEBinary = chcnpmiVMAEBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiVMAEtfidf = CoherenceModel(topics=topicsVMAEtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMAEtfidf = chcnpmiVMAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcnpmiVMRBMBinary = CoherenceModel(topics=topicsVMRBMBinary, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMRBMBinary = chcnpmiVMRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiVMRBMtfidf = CoherenceModel(topics=topicsVMRBMtfidf, texts=tokenVM, dictionary=dictVM, coherence='c_npmi')
cnpmiVMRBMtfidf = chcnpmiVMRBMtfidf.get_coherence()  # get coherence value

"""### 9.2.5 Autor 4"""

#Analisis de coherencia SVD
#Binary
cmIGSVDBinary = CoherenceModel(topics=topicsIGSVDBinary, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGSVDBinary = cmIGSVDBinary.get_coherence()  # get coherence value
#TF IDF
cmIGSVDtfidf = CoherenceModel(topics=topicsIGSVDtfidf, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGSVDtfidf = cmIGSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
cmIGPLSA = CoherenceModel(topics=topicsIGPLSA, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGPLSA = cmIGPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
cmIGLDABinary = CoherenceModel(topics=topicsIGLDABinary, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGLDABinary = cmIGLDABinary.get_coherence()  # get coherence value

#TF IDF
cmIGLDAtfidf = CoherenceModel(topics=topicsIGLDAtfidf, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGLDAtfidf = cmIGLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
cmIGAEBinary = CoherenceModel(topics=topicsIGAEBinary, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGAEBinary = cmIGAEBinary.get_coherence()  # get coherence value

#TF IDF
cmIGAEtfidf = CoherenceModel(topics=topicsIGAEtfidf, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGAEtfidf = cmIGAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
cmIGRBMBinary = CoherenceModel(topics=topicsIGRBMBinary, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGRBMBinary = cmIGRBMBinary.get_coherence()  # get coherence value
#TF IDF
cmIGRBMtfidf = CoherenceModel(topics=topicsIGRBMtfidf, corpus=corpusIG, dictionary=dictIG, coherence='u_mass')
coherenceIGRBMtfidf = cmIGRBMtfidf.get_coherence()  # get coherence value

# C_V
#Analisis de coherencia SVD
#Binary
chcvIGSVDBinary = CoherenceModel(topics=topicsIGSVDBinary, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGSVDBinary = chcvIGSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcvIGSVDtfidf = CoherenceModel(topics=topicsIGSVDtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGSVDtfidf = chcvIGSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcvIGPLSA = CoherenceModel(topics=topicsIGPLSA, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGPLSA = chcvIGPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcvIGLDABinary = CoherenceModel(topics=topicsIGLDABinary, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGLDABinary = chcvIGLDABinary.get_coherence()  # get coherence value
#TF IDF
chcvIGLDAtfidf = CoherenceModel(topics=topicsIGLDAtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGLDAtfidf = chcvIGLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcvIGAEBinary = CoherenceModel(topics=topicsIGAEBinary, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGAEBinary = chcvIGAEBinary.get_coherence()  # get coherence value
#TF IDF
chcvIGAEtfidf = CoherenceModel(topics=topicsIGAEtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGAEtfidf = chcvIGAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcvIGRBMBinary = CoherenceModel(topics=topicsIGRBMBinary, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGRBMBinary = chcvIGRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcvIGRBMtfidf = CoherenceModel(topics=topicsIGRBMtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_v')
cvIGRBMtfidf = chcvIGRBMtfidf.get_coherence()  # get coherence value

# c_uci
#Analisis de coherencia SVD
#Binary
chcuciIGSVDBinary = CoherenceModel(topics=topicsIGSVDBinary, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGSVDBinary = chcuciIGSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcuciIGSVDtfidf = CoherenceModel(topics=topicsIGSVDtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGSVDtfidf = chcuciIGSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcuciIGPLSA = CoherenceModel(topics=topicsIGPLSA, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGPLSA = chcuciIGPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcuciIGLDABinary = CoherenceModel(topics=topicsIGLDABinary, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGLDABinary = chcuciIGLDABinary.get_coherence()  # get coherence value
#TF IDF
chcuciIGLDAtfidf = CoherenceModel(topics=topicsIGLDAtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGLDAtfidf = chcuciIGLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcuciIGAEBinary = CoherenceModel(topics=topicsIGAEBinary, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGAEBinary = chcuciIGAEBinary.get_coherence()  # get coherence value
#TF IDF
chcuciIGAEtfidf = CoherenceModel(topics=topicsIGAEtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGAEtfidf = chcuciIGAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcuciIGRBMBinary = CoherenceModel(topics=topicsIGRBMBinary, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGRBMBinary = chcuciIGRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcuciIGRBMtfidf = CoherenceModel(topics=topicsIGRBMtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_uci')
cuciIGRBMtfidf = chcuciIGRBMtfidf.get_coherence()  # get coherence value

# c_npmi
#Analisis de coherencia SVD
#Binary
chcnpmiIGSVDBinary = CoherenceModel(topics=topicsIGSVDBinary, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGSVDBinary = chcnpmiIGSVDBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiIGSVDtfidf = CoherenceModel(topics=topicsIGSVDtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGSVDtfidf = chcnpmiIGSVDtfidf.get_coherence()  # get coherence value

#Analisis de coherencia PLSA
chcnpmiIGPLSA = CoherenceModel(topics=topicsIGPLSA, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGPLSA = chcnpmiIGPLSA.get_coherence()  # get coherence value

#Analisis de coherencia LDA
#Binary
chcnpmiIGLDABinary = CoherenceModel(topics=topicsIGLDABinary, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGLDABinary = chcnpmiIGLDABinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiIGLDAtfidf = CoherenceModel(topics=topicsIGLDAtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGLDAtfidf = chcnpmiIGLDAtfidf.get_coherence()  # get coherence value

#Analisis de coherencia autoencoder
#Binary
chcnpmiIGAEBinary = CoherenceModel(topics=topicsIGAEBinary, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGAEBinary = chcnpmiIGAEBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiIGAEtfidf = CoherenceModel(topics=topicsIGAEtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGAEtfidf = chcnpmiIGAEtfidf.get_coherence()  # get coherence value

#Analisis de coherencia RBM
#Binary
chcnpmiIGRBMBinary = CoherenceModel(topics=topicsIGRBMBinary, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGRBMBinary = chcnpmiIGRBMBinary.get_coherence()  # get coherence value
#TF IDF
chcnpmiIGRBMtfidf = CoherenceModel(topics=topicsIGRBMtfidf, texts=tokenIG, dictionary=dictIG, coherence='c_npmi')
cnpmiIGRBMtfidf = chcnpmiIGRBMtfidf.get_coherence()  # get coherence value

"""# Resultados"""

##Resultados
print("Resultados de métrica de coherencia")
print("Autor 1")
print("u_mass| SVD binary|", coherenceDCSVDBinary)
print("u_mass| SVD tfidf|", coherenceDCSVDtfidf)
print("u_mass| PLSA|", coherenceDCPLSA)
print("u_mass| LDA binary|", coherenceDCLDABinary)
print("u_mass| LDA tfidf|", coherenceDCLDAtfidf)
print("u_mass| Autoencoder binary|", coherenceDCAEBinary)
print("u_mass| Autoencoder tfidf|", coherenceDCAEtfidf)
print("u_mass| RBM binary|", coherenceDCRBMBinary)
print("u_mass| RBM tfidf|", coherenceDCRBMtfidf)
print("c_v| SVD binary|", cvDCSVDBinary)
print("c_v| SVD tfidf|", cvDCSVDtfidf)
print("c_v| PLSA|", cvDCPLSA)
print("c_v| LDA binary|", cvDCLDABinary)
print("c_v| LDA tfidf|", cvDCLDAtfidf)
print("c_v| Autoencoder binary|", cvDCAEBinary)
print("c_v| Autoencoder tfidf|", cvDCAEtfidf)
print("c_v| RBM binary|", cvDCRBMBinary)
print("c_v| RBM tfidf|", cvDCRBMtfidf)
print("c_uci| SVD binary|", cuciDCSVDBinary)
print("c_uci| SVD tfidf|", cuciDCSVDtfidf)
print("c_uci| PLSA|", cuciDCPLSA)
print("c_uci| LDA binary|", cuciDCLDABinary)
print("c_uci| LDA tfidf|", cuciDCLDAtfidf)
print("c_uci| Autoencoder binary|", cuciDCAEBinary)
print("c_uci| Autoencoder tfidf|", cuciDCAEtfidf)
print("c_uci| RBM binary|", cuciDCRBMBinary)
print("c_uci| RBM tfidf|", cuciDCRBMtfidf)
print("c_npmi| SVD binary|", cnpmiDCSVDBinary)
print("c_npmi| SVD tfidf|", cnpmiDCSVDtfidf)
print("c_npmi| PLSA|", cnpmiDCPLSA)
print("c_npmi| LDA binary|", cnpmiDCLDABinary)
print("c_npmi| LDA tfidf|", cnpmiDCLDAtfidf)
print("c_npmi| Autoencoder binary|", cnpmiDCAEBinary)
print("c_npmi| Autoencoder tfidf|", cnpmiDCAEtfidf)
print("c_npmi| RBM binary|", cnpmiDCRBMBinary)
print("c_npmi| RBM tfidf|", cnpmiDCRBMtfidf)
print(" ")
print("Autor 2")
print("u_mass| SVD binary|", coherenceEDSVDBinary)
print("u_mass| SVD tfidf|", coherenceEDSVDtfidf)
print("u_mass| PLSA|", coherenceEDPLSA)
print("u_mass| LDA binary|", coherenceEDLDABinary)
print("u_mass| LDA tfidf|", coherenceEDLDAtfidf)
print("u_mass| Autoencoder binary|", coherenceEDAEBinary)
print("u_mass| Autoencoder tfidf|", coherenceEDAEtfidf)
print("u_mass| RBM binary|", coherenceEDRBMBinary)
print("u_mass| RBM tfidf|", coherenceEDRBMtfidf)
print("c_v| SVD binary|", cvEDSVDBinary)
print("c_v| SVD tfidf|", cvEDSVDtfidf)
print("c_v| PLSA|", cvEDPLSA)
print("c_v| LDA binary|", cvEDLDABinary)
print("c_v| LDA tfidf|", cvEDLDAtfidf)
print("c_v| Autoencoder binary|", cvEDAEBinary)
print("c_v| Autoencoder tfidf|", cvEDAEtfidf)
print("c_v| RBM binary|", cvEDRBMBinary)
print("c_v| RBM tfidf|", cvEDRBMtfidf)
print("c_uci| SVD binary|", cuciEDSVDBinary)
print("c_uci| SVD tfidf|", cuciEDSVDtfidf)
print("c_uci| PLSA|", cuciEDPLSA)
print("c_uci| LDA binary|", cuciEDLDABinary)
print("c_uci| LDA tfidf|", cuciEDLDAtfidf)
print("c_uci| Autoencoder binary|", cuciEDAEBinary)
print("c_uci| Autoencoder tfidf|", cuciEDAEtfidf)
print("c_uci| RBM binary|", cuciEDRBMBinary)
print("c_uci| RBM tfidf|", cuciEDRBMtfidf)
print("c_npmi| SVD binary|", cnpmiEDSVDBinary)
print("c_npmi| SVD tfidf|", cnpmiEDSVDtfidf)
print("c_npmi| PLSA|", cnpmiEDPLSA)
print("c_npmi| LDA binary|", cnpmiEDLDABinary)
print("c_npmi| LDA tfidf|", cnpmiEDLDAtfidf)
print("c_npmi| Autoencoder binary|", cnpmiEDAEBinary)
print("c_npmi| Autoencoder tfidf|", cnpmiEDAEtfidf)
print("c_npmi| RBM binary|", cnpmiEDRBMBinary)
print("c_npmi| RBM tfidf|", cnpmiEDRBMtfidf)
print(" ")
print("Autor 3")
print("u_mass| SVD binary|", coherenceVMSVDBinary)
print("u_mass| SVD tfidf|", coherenceVMSVDtfidf)
print("u_mass| PLSA|", coherenceVMPLSA)
print("u_mass| LDA binary|", coherenceVMLDABinary)
print("u_mass| LDA tfidf|", coherenceVMLDAtfidf)
print("u_mass| Autoencoder binary|", coherenceVMAEBinary)
print("u_mass| Autoencoder tfidf|", coherenceVMAEtfidf)
print("u_mass| RBM binary|", coherenceVMRBMBinary)
print("u_mass| RBM tfidf|", coherenceVMRBMtfidf)
print("c_v| SVD binary|", cvVMSVDBinary)
print("c_v| SVD tfidf|", cvVMSVDtfidf)
print("c_v| PLSA|", cvVMPLSA)
print("c_v| LDA binary|", cvVMLDABinary)
print("c_v| LDA tfidf|", cvVMLDAtfidf)
print("c_v| Autoencoder binary|", cvVMAEBinary)
print("c_v| Autoencoder tfidf|", cvVMAEtfidf)
print("c_v| RBM binary|", cvVMRBMBinary)
print("c_v| RBM tfidf|", cvVMRBMtfidf)
print("c_uci| SVD binary|", cuciVMSVDBinary)
print("c_uci| SVD tfidf|", cuciVMSVDtfidf)
print("c_uci| PLSA|", cuciVMPLSA)
print("c_uci| LDA binary|", cuciVMLDABinary)
print("c_uci| LDA tfidf|", cuciVMLDAtfidf)
print("c_uci| Autoencoder binary|", cuciVMAEBinary)
print("c_uci| Autoencoder tfidf|", cuciVMAEtfidf)
print("c_uci| RBM binary|", cuciVMRBMBinary)
print("c_uci| RBM tfidf|", cuciVMRBMtfidf)
print("c_npmi| SVD binary|", cnpmiVMSVDBinary)
print("c_npmi| SVD tfidf|", cnpmiVMSVDtfidf)
print("c_npmi| PLSA|", cnpmiVMPLSA)
print("c_npmi| LDA binary|", cnpmiVMLDABinary)
print("c_npmi| LDA tfidf|", cnpmiVMLDAtfidf)
print("c_npmi| Autoencoder binary|", cnpmiVMAEBinary)
print("c_npmi| Autoencoder tfidf|", cnpmiVMAEtfidf)
print("c_npmi| RBM binary|", cnpmiVMRBMBinary)
print("c_npmi| RBM tfidf|", cnpmiVMRBMtfidf)
print(" ")
print("Autor 4")
print("u_mass| SVD binary|", coherenceIGSVDBinary)
print("u_mass| SVD tfidf|", coherenceIGSVDtfidf)
print("u_mass| PLSA|", coherenceIGPLSA)
print("u_mass| LDA binary|", coherenceIGLDABinary)
print("u_mass| LDA tfidf|", coherenceIGLDAtfidf)
print("u_mass| Autoencoder binary|", coherenceIGAEBinary)
print("u_mass| Autoencoder tfidf|", coherenceIGAEtfidf)
print("u_mass| RBM binary|", coherenceIGRBMBinary)
print("u_mass| RBM tfidf|", coherenceIGRBMtfidf)
print("c_v| SVD binary|", cvIGSVDBinary)
print("c_v| SVD tfidf|", cvIGSVDtfidf)
print("c_v| PLSA|", cvIGPLSA)
print("c_v| LDA binary|", cvIGLDABinary)
print("c_v| LDA tfidf|", cvIGLDAtfidf)
print("c_v| Autoencoder binary|", cvIGAEBinary)
print("c_v| Autoencoder tfidf|", cvIGAEtfidf)
print("c_v| RBM binary|", cvIGRBMBinary)
print("c_v| RBM tfidf|", cvIGRBMtfidf)
print("c_uci| SVD binary|", cuciIGSVDBinary)
print("c_uci| SVD tfidf|", cuciIGSVDtfidf)
print("c_uci| PLSA|", cuciIGPLSA)
print("c_uci| LDA binary|", cuciIGLDABinary)
print("c_uci| LDA tfidf|", cuciIGLDAtfidf)
print("c_uci| Autoencoder binary|", cuciIGAEBinary)
print("c_uci| Autoencoder tfidf|", cuciIGAEtfidf)
print("c_uci| RBM binary|", cuciIGRBMBinary)
print("c_uci| RBM tfidf|", cuciIGRBMtfidf)
print("c_npmi| SVD binary|", cnpmiIGSVDBinary)
print("c_npmi| SVD tfidf|", cnpmiIGSVDtfidf)
print("c_npmi| PLSA|", cnpmiIGPLSA)
print("c_npmi| LDA binary|", cnpmiIGLDABinary)
print("c_npmi| LDA tfidf|", cnpmiIGLDAtfidf)
print("c_npmi| Autoencoder binary|", cnpmiIGAEBinary)
print("c_npmi| Autoencoder tfidf|", cnpmiIGAEtfidf)
print("c_npmi| RBM binary|", cnpmiIGRBMBinary)
print("c_npmi| RBM tfidf|", cnpmiIGRBMtfidf)
print(" ")

"""#10. **Comparativo**"""

#Ver tópicos por cada autor
print("Autor 1")
print("SVD Binary")
print(topicsDCSVDBinary)
print("SVD tf idf")
print(topicsDCSVDtfidf)
print("pLSA")
print(topicsDCPLSA)
print("LDA Binary")
print(topicsDCLDABinary)
print("LDA tf idf")
print(topicsDCLDAtfidf)
print("Autoencoder Binary")
print(topicsDCAEBinary)
print("Autoencoder tf idf")
print(topicsDCAEtfidf)
print("RBM Binary")
print(topicsDCRBMBinary)
print("RBM tf idf")
print(topicsDCRBMtfidf)
print(" ")
print("Autor 2")
print("SVD Binary")
print(topicsEDSVDBinary)
print("SVD tf idf")
print(topicsEDSVDtfidf)
print("pLSA")
print(topicsEDPLSA)
print("LDA Binary")
print(topicsEDLDABinary)
print("LDA tf idf")
print(topicsEDLDAtfidf)
print("Autoencoder Binary")
print(topicsEDAEBinary)
print("Autoencoder tf idf")
print(topicsEDAEtfidf)
print("RBM Binary")
print(topicsEDRBMBinary)
print("RBM tf idf")
print(topicsEDRBMtfidf)
print(" ")
print("Autor 3")
print("SVD Binary")
print(topicsVMSVDBinary)
print("SVD tf idf")
print(topicsVMSVDtfidf)
print("pLSA")
print(topicsVMPLSA)
print("LDA Binary")
print(topicsVMLDABinary)
print("LDA tf idf")
print(topicsVMLDAtfidf)
print("Autoencoder Binary")
print(topicsVMAEBinary)
print("Autoencoder tf idf")
print(topicsVMAEtfidf)
print("RBM Binary")
print(topicsVMRBMBinary)
print("RBM tf idf")
print(topicsVMRBMtfidf)
print(" ")
print("Autor 4")
print("SVD Binary")
print(topicsIGSVDBinary)
print("SVD tf idf")
print(topicsIGSVDtfidf)
print("pLSA")
print(topicsIGPLSA)
print("LDA Binary")
print(topicsIGLDABinary)
print("LDA tf idf")
print(topicsIGLDAtfidf)
print("Autoencoder Binary")
print(topicsIGAEBinary)
print("Autoencoder tf idf")
print(topicsIGAEtfidf)
print("RBM Binary")
print(topicsIGRBMBinary)
print("RBM tf idf")
print(topicsIGRBMtfidf)

tiempos

"""Documentación pLSA: https://github.com/yedivanseven/PLSA/blob/master/notebooks/Examples.ipynb

https://www.programmersought.com/article/18624933107/

https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41
"""